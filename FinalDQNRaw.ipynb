{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalDQNRaw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Python version: 3.7.5 (default, Oct 31 2019, 15:18:51) [MSC v.1916 64 bit (AMD64)]\nTensorflow version: 2.0.0\nFound GPU at: /device:GPU:0\nNum GPUs Available:  1\n"
        }
      ],
      "source": [
        "# Libraries \n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import io\n",
        "import base64\n",
        "from gym import wrappers\n",
        "from IPython import display\n",
        "from IPython.display import HTML\n",
        "%matplotlib inline\n",
        "\n",
        "# Install TF 2 and enable GPU\n",
        "if \"2.\" not in tf.__version__ or not tf.test.is_gpu_available(): \n",
        "  !pip uninstall tensorflow\n",
        "  !pip install tensorflow-gpu\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyper parameters \n",
        "PROBLEM = 'BreakoutDeterministic-v4'\n",
        "FRAME_SKIP = 4\n",
        "MEMORY_BATCH_SIZE = 32\n",
        "REPLAY_START_SIZE = 50000\n",
        "REPLAY_MEMORY_SIZE = 1000000  # RMSProp train updates sampled from this number of recent frames\n",
        "NUMBER_OF_EPISODES = 1000000  # TODO: save and restore model with infinite episodes\n",
        "EXPLORATION_RATE = 1\n",
        "MIN_EXPLORATION_RATE = 0.1\n",
        "MAX_FRAMES_DECAYED = REPLAY_MEMORY_SIZE / FRAME_SKIP  # TODO: correct? 1 million in paper\n",
        "IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS = 84, 84, 1  \n",
        "IMAGE_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS) \n",
        "CONV1_NUM_FILTERS, CONV1_FILTER_SIZE, CONV1_FILTER_STRIDES = 32, 8, 4\n",
        "CONV2_NUM_FILTERS, CONV2_FILTER_SIZE, CONV2_FILTER_STRIDES = 64, 4, 2\n",
        "CONV3_NUM_FILTERS, CONV3_FILTER_SIZE, CONV3_FILTER_STRIDES = 64, 3, 1\n",
        "DENSE_NUM_UNITS, OUTPUT_NUM_UNITS = 512, 4  # TODO: GET Action count from constructor\n",
        "LEARNING_RATE, GRADIENT_MOMENTUM, MIN_SQUARED_GRADIENT = 0.00025, 0.95, 0.01\n",
        "HUBER_LOSS_DELTA, DISCOUNT_FACTOR = 1.0, 0.99  \n",
        "RANDOM_WEIGHT_INITIALIZER = tf.initializers.RandomNormal()\n",
        "HIDDEN_ACTIVATION, OUTPUT_ACTIVATION, PADDING = 'relu', 'linear', \"SAME\"  # TODO: remove?\n",
        "TARGET_MODEL_UPDATE_FREQUENCY = 10000\n",
        "optimizer = tf.optimizers.RMSprop(learning_rate=LEARNING_RATE, rho=GRADIENT_MOMENTUM, epsilon=MIN_SQUARED_GRADIENT)\n",
        "# LEAKY_RELU_ALPHA, DROPOUT_RATE = 0.2, 0.5  # TODO: remove or use to improve paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FramePreprocessor:\n",
        "    \"\"\"\n",
        "    FramePreprocessor re-sizes, normalizes and converts RGB atari frames to gray scale frames.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_space):\n",
        "        self.state_space = state_space\n",
        "\n",
        "    def convert_rgb_to_grayscale(self, tf_frame):\n",
        "        return tf.image.rgb_to_grayscale(tf_frame)\n",
        "    \n",
        "    def resize_frame(self, tf_frame, frame_height, frame_width):\n",
        "        return tf.image.resize(tf_frame, [frame_height,frame_width], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "\n",
        "    def plot_frame_from_greyscale_values(self, image):\n",
        "        height, width, _ = image.shape \n",
        "        grey_image = np.array([[(image[i, j].numpy()[0], image[i, j].numpy()[0], image[i, j].numpy()[0]) \n",
        "                                for i in range(height)]\n",
        "                                for j in range(width)])\n",
        "        grey_image = np.transpose(grey_image, (1, 0, 2))  # Switch height and width\n",
        "        plt.imshow(grey_image) \n",
        "        plt.show()\n",
        "\n",
        "    def preprocess_frame(self, frame):\n",
        "        tf_frame = tf.Variable(frame, shape=self.state_space, dtype=tf.uint8)\n",
        "        image = self.convert_rgb_to_grayscale(tf_frame)\n",
        "        image = self.resize_frame(image, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Todo use experience: (state, action, reward, next_state, is_done)\n",
        "from typing import NamedTuple, Tuple \n",
        "class Experience(NamedTuple): \n",
        "  state: Tuple[int, int, int] # y, x, c\n",
        "  action: int \n",
        "  reward: float \n",
        "  next_state: Tuple[int, int, int]\n",
        "  is_done: bool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    Memory class holds a list of game plays stored as experiences (s,a,r,s', d) = (state, action, reward, next_state, is_done)\n",
        "    Credits: https://stackoverflow.com/questions/40181284/how-to-get-random-sample-from-deque-in-python-3 \n",
        "    \"\"\"\n",
        "    def __init__(self, capacity):  # Initialize memory with given capacity\n",
        "        self.experiences = [None] * capacity\n",
        "        self.capacity = capacity\n",
        "        self.index = 0\n",
        "        self.size = 0 \n",
        "    \n",
        "    def add(self, experience): # Add a sample to the memory, removing the earliest entry if memeory capacity is reached\n",
        "      self.experiences[self.index] = experience \n",
        "      self.size = min(self.size + 1, self.capacity)\n",
        "      self.index = (self.index + 1) % self.capacity  # Overwrites earliest entry if memory capacity reached\n",
        "\n",
        "    def sample(self, size): \n",
        "      indices = random.sample(range(self.size), size)\n",
        "      return [self.experiences[index] for index in indices]  # Efficient random access"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvolutionalNeuralNetwork:\n",
        "    \"\"\"\n",
        "    CNN CLASS\n",
        "    Architecture of DQN has 4 hidden layers:\n",
        "\n",
        "    Input:  84 X 84 X 1 image (4 in paper due to frame skipping) (PREPROCESSED image), Game-score, Life count, Actions_count (4)\n",
        "    1st Hidden layer: Convolves 32 filters of 8 X 8 with stride 4 (relu)\n",
        "    2nd hidden layer: Convolves 64 filters of 4 X 4 with stride 2 (relu)\n",
        "    3rd hidden layer: Convolves 64 filters of 3 X 3 with stride 1 (Relu)\n",
        "    4th hidden layer: Fully connected, (512 relu units)\n",
        "    Output: Fully connected linear layer, Separate output unit for each action, outputs are predicted Q-values\n",
        "    \"\"\"\n",
        "\n",
        "    weights = { # 4D: Filter Width, Height, In Channel, Out Channel  \n",
        "        # Conv Layer 1: 8x8 conv, 1 input (preprocessed image has 1 color channel), 32 output filters\n",
        "        'conv1_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([CONV1_FILTER_SIZE, CONV1_FILTER_SIZE, IMAGE_CHANNELS, CONV1_NUM_FILTERS])),  \n",
        "        # Conv Layer 2: 4x4 conv, 32 input filters, 64 output filters\n",
        "        'conv2_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([CONV2_FILTER_SIZE, CONV2_FILTER_SIZE, CONV1_NUM_FILTERS, CONV2_NUM_FILTERS])),\n",
        "        # Conv Layer 3: 3x3 conv, 64 input filters, 64 output filters\n",
        "        'conv3_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([CONV3_FILTER_SIZE, CONV3_FILTER_SIZE, CONV2_NUM_FILTERS, CONV3_NUM_FILTERS])),\n",
        "        # Fully Connected (Dense) Layer: 3x3x64 inputs (64 filters of size 3x3), 512 output units\n",
        "        'dense_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([IMAGE_HEIGHT * IMAGE_WIDTH * CONV3_NUM_FILTERS, DENSE_NUM_UNITS])),\n",
        "        # Output layer: 512 input units, 4 output units (actions)\n",
        "        'output_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([DENSE_NUM_UNITS, OUTPUT_NUM_UNITS]))\n",
        "    }\n",
        "\n",
        "    biases = {\n",
        "        'conv1_biases': tf.Variable(tf.zeros([CONV1_NUM_FILTERS])),  # 32\n",
        "        'conv2_biases': tf.Variable(tf.zeros([CONV2_NUM_FILTERS])),  # 64\n",
        "        'conv3_biases': tf.Variable(tf.zeros([CONV3_NUM_FILTERS])),  # 64\n",
        "        'dense_biases': tf.Variable(tf.zeros([DENSE_NUM_UNITS])),  # 512\n",
        "        'output_biases': tf.Variable(tf.zeros([OUTPUT_NUM_UNITS]))  # 4\n",
        "    }\n",
        "\n",
        "    target_weights = {\n",
        "        # Conv Layer 1: 8x8 conv, 1 input (preprocessed image has 1 color channel), 32 output filters\n",
        "        'conv1_target_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([CONV1_FILTER_SIZE, CONV1_FILTER_SIZE, IMAGE_CHANNELS, CONV1_NUM_FILTERS])),\n",
        "        # Conv Layer 2: 4x4 conv, 32 input filters, 64 output filters\n",
        "        'conv2_target_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([CONV2_FILTER_SIZE, CONV2_FILTER_SIZE, CONV1_NUM_FILTERS, CONV2_NUM_FILTERS])),\n",
        "        # Conv Layer 3: 3x3 conv, 64 input filters, 64 output filters\n",
        "        'conv3_target_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([CONV3_FILTER_SIZE, CONV3_FILTER_SIZE, CONV2_NUM_FILTERS, CONV3_NUM_FILTERS])),\n",
        "        # Fully Connected (Dense) Layer: 3x3x64 inputs (64 filters of size 3x3), 512 output units\n",
        "        'dense_target_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([IMAGE_HEIGHT * IMAGE_WIDTH * CONV3_NUM_FILTERS, DENSE_NUM_UNITS])),\n",
        "        # Output layer: 512 input units, 4 output units (actions)\n",
        "        'output_target_weights': tf.Variable(RANDOM_WEIGHT_INITIALIZER([DENSE_NUM_UNITS, OUTPUT_NUM_UNITS]))\n",
        "    }\n",
        "\n",
        "    target_biases = {\n",
        "        'conv1_target_biases': tf.Variable(tf.zeros([CONV1_NUM_FILTERS])),  # 32\n",
        "        'conv2_target_biases': tf.Variable(tf.zeros([CONV2_NUM_FILTERS])),  # 64\n",
        "        'conv3_target_biases': tf.Variable(tf.zeros([CONV3_NUM_FILTERS])),  # 64\n",
        "        'dense_target_biases': tf.Variable(tf.zeros([DENSE_NUM_UNITS])),  # 512\n",
        "        'output_target_biases': tf.Variable(tf.zeros([OUTPUT_NUM_UNITS]))  # 4\n",
        "    }\n",
        "\n",
        "    def __init__(self, number_of_states, number_of_actions):  #, model=None):\n",
        "      self.number_of_states = number_of_states\n",
        "      self.number_of_actions = number_of_actions\n",
        "\n",
        "    @tf.function\n",
        "    def overwrite_model_params(self): # Assume same order and length \n",
        "      for weight, target_weight_key in zip(self.weights.values(), self.target_weights.keys()): \n",
        "        self.target_weights[target_weight_key].assign(tf.identity(weight))\n",
        "\n",
        "      for bias, target_bias_key in zip(self.biases.values(), self.target_biases.keys()): \n",
        "        self.target_biases[target_bias_key].assign(tf.identity(bias)) \n",
        "        \n",
        "    @tf.function\n",
        "    def normalize_images(self, images):\n",
        "        return tf.cast(images / 255, dtype=tf.float32)\n",
        "\n",
        "    @tf.function\n",
        "    def convolutional_2d_layer(self, inputs, filter_weights, biases, strides=1):\n",
        "        output = tf.nn.conv2d(inputs, filter_weights, strides, padding=PADDING)  # TODO: padding in paper?\n",
        "        output_with_bias = tf.nn.bias_add(output, biases)\n",
        "        activation = tf.nn.relu(output_with_bias)  # non-linearity TODO: improve paper with leaky relu?\n",
        "        return activation\n",
        "\n",
        "    @tf.function\n",
        "    def flatten_layer(self, layer):  # output shape: [32, 64*84*84]\n",
        "        # Shape: Minibatches: 32, Num of Filters * Img Height, Image width: 64*84*84 = 451584\n",
        "        memory_batch_size, image_height, image_width, num_filters = layer.get_shape()\n",
        "        flattened_layer = tf.reshape(layer, (memory_batch_size, num_filters * image_height * image_width))\n",
        "        return flattened_layer\n",
        "\n",
        "    @tf.function\n",
        "    def dense_layer(self, inputs, weights, biases):\n",
        "        output = tf.nn.bias_add(tf.matmul(inputs, weights), biases)\n",
        "        dense_activation = tf.nn.relu(output)  # non-linearity\n",
        "        # dropout = tf.nn.dropout(dense_activation, rate=DROPOUT_RATE)  # TODO: does paper dropout?\n",
        "        return dense_activation\n",
        "\n",
        "    @tf.function\n",
        "    def output_layer(self, input, weights, biases):\n",
        "        linear_output = tf.nn.bias_add(tf.matmul(input, weights), biases)\n",
        "        return linear_output\n",
        "\n",
        "    @tf.function\n",
        "    def huber_error_loss(self, y_true, y_predictions, delta=1.0):\n",
        "            y_predictions = tf.cast(y_predictions, dtype=tf.float32)\n",
        "            errors = y_true - y_predictions\n",
        "            condition = tf.abs(errors) <= delta\n",
        "            l2_squared_loss = 0.5 * tf.square(errors)\n",
        "            l1_absolute_loss = delta * (tf.abs(errors) - 0.5 * delta)\n",
        "            loss = tf.where(condition, l2_squared_loss, l1_absolute_loss)\n",
        "            return loss\n",
        "\n",
        "    @tf.function\n",
        "    def train(self, inputs, outputs):  # Optimization\n",
        "        # Wrap computation inside a GradientTape for automatic differentiation\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.predict(inputs)\n",
        "            current_loss = self.huber_error_loss(predictions, outputs)\n",
        "\n",
        "        # Trainable variables to update\n",
        "        trainable_variables = list(self.weights.values()) + list(self.biases.values())\n",
        "\n",
        "        gradients = tape.gradient(current_loss, trainable_variables)\n",
        "\n",
        "        # Update weights and biases following gradients\n",
        "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "        # tf.print(tf.reduce_mean(current_loss))\n",
        "      \n",
        "    @tf.function\n",
        "    def predict(self, inputs, is_target = False): # 4D input for CNN: (batch_size, height, width, depth) \n",
        "        # Input shape: [32, 84, 84, 1]. A batch of 84x84x1 (gray scale) images.        \n",
        "        # inputs = self.normalize_images(inputs) # TODO \n",
        "        inputs = tf.cast(inputs, tf.float32)\n",
        "\n",
        "        # Convolution Layer 1 with output shape [32, 84, 84, 32]\n",
        "        conv1_weights = self.target_weights['conv1_target_weights'] if is_target else self.weights['conv1_weights']\n",
        "        conv1_biases = self.target_biases['conv1_target_biases'] if is_target else self.biases['conv1_biases']\n",
        "        conv1 = self.convolutional_2d_layer(inputs,conv1_weights,conv1_biases)\n",
        "\n",
        "        # Convolutional Layer 2 with output shape [32, 84, 84, 64]\n",
        "        conv2_weights = self.target_weights['conv2_target_weights'] if is_target else self.weights['conv2_weights']\n",
        "        conv2_biases = self.target_biases['conv2_target_biases'] if is_target else self.biases['conv2_biases']\n",
        "        conv2 = self.convolutional_2d_layer(conv1, conv2_weights, conv2_biases)\n",
        "\n",
        "        # Convolutional Layer 3 with output shape [1, 84, 84, 64]\n",
        "        conv3_weights = self.target_weights['conv3_target_weights'] if is_target else self.weights['conv3_weights']\n",
        "        conv3_biases = self.target_biases['conv3_target_biases'] if is_target else self.biases['conv3_biases']\n",
        "        conv3 = self.convolutional_2d_layer(conv2, conv3_weights, conv3_biases)\n",
        "\n",
        "        # Flatten output of 2nd conv. layer to fit dense layer input, output shape [32, 64*84*84]\n",
        "        flattened_layer = self.flatten_layer(layer=conv3) \n",
        "\n",
        "        # Dense fully connected layer with output shape [1, 512]\n",
        "        dense_weights = self.target_weights['dense_target_weights'] if is_target else self.weights['dense_weights']\n",
        "        dense_biases = self.target_biases['dense_target_biases'] if is_target else self.biases['dense_biases']\n",
        "\n",
        "        dense_layer = self.dense_layer(flattened_layer, dense_weights, dense_biases)\n",
        "\n",
        "        # Fully connected output of shape [1, 4]\n",
        "        output_weights = self.target_weights['output_target_weights'] if is_target else self.weights['output_weights']\n",
        "        output_biases = self.target_biases['output_target_biases'] if is_target else self.biases['output_biases']\n",
        "        output_layer = self.output_layer(dense_layer, output_weights, output_biases)\n",
        "\n",
        "        return output_layer\n",
        "\n",
        "    @tf.function\n",
        "    def predict_one(self, state, is_target = False):\n",
        "        state = tf.reshape(state, shape=(1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)) # Reshape \n",
        "        prediction = self.predict(state, is_target)\n",
        "        return prediction "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agent takes actions and saves them to its memory, which is initialized with a given capacity\n",
        "    \"\"\"\n",
        "    steps = 0\n",
        "    exploration_rate = EXPLORATION_RATE\n",
        "\n",
        "    def decay_exploration_rate(self):\n",
        "        decay_rate = (self.exploration_rate - MIN_EXPLORATION_RATE) / MAX_FRAMES_DECAYED\n",
        "        return decay_rate\n",
        "\n",
        "    # Initialize agent with a given memory capacity, and a state, and action space\n",
        "    def __init__(self, number_of_states, number_of_actions, model=None):\n",
        "        self.experiences = ReplayMemory(REPLAY_MEMORY_SIZE)\n",
        "        self.model = ConvolutionalNeuralNetwork(number_of_states, number_of_actions)  #, model) if model else ConvolutionalNeuralNetwork(number_of_states, number_of_actions)\n",
        "        self.number_of_states = number_of_states\n",
        "        self.number_of_actions = number_of_actions\n",
        "        self.decay_rate = self.decay_exploration_rate()\n",
        "\n",
        "    # The behaviour policy during training was e-greedy with e annealed linearly\n",
        "    # from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter\n",
        "    def e_greedy_policy(self, state):\n",
        "        exploration_rate_threshold = random.uniform(0, 1)\n",
        "        if exploration_rate_threshold > self.exploration_rate:\n",
        "            next_q_values = self.model.predict_one(state)\n",
        "            best_action = np.argmax(next_q_values) # tf.argmax returns multiple indices in tie \n",
        "        else:\n",
        "            best_action = self.random_policy() \n",
        "        return best_action\n",
        "\n",
        "    def random_policy(self):\n",
        "        return random.randint(0, self.number_of_actions - 1)\n",
        "\n",
        "    def act(self, state):\n",
        "        return self.random_policy() if self.experiences.size <= REPLAY_START_SIZE else self.e_greedy_policy(state)\n",
        "\n",
        "    def update_target_model(self):\n",
        "      self.model.overwrite_model_params()\n",
        "      \n",
        "    @tf.function \n",
        "    def reshape_image(self, images, batch_size=1): \n",
        "      return tf.reshape(images, shape=(batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))\n",
        "\n",
        "    def observe(self, experience):\n",
        "        self.experiences.add(experience)\n",
        "        self.steps += 1\n",
        "        self.exploration_rate = (MIN_EXPLORATION_RATE if self.exploration_rate <= MIN_EXPLORATION_RATE\n",
        "                                 else self.exploration_rate - self.decay_rate)\n",
        "        \n",
        "        if self.steps % TARGET_MODEL_UPDATE_FREQUENCY == 0:\n",
        "          self.update_target_model()\n",
        "\n",
        "    def replay(self):  # Experience: (state, action, reward, next_state, is_done) # Train neural net with experiences\n",
        "        memory_batch = self.experiences.sample(MEMORY_BATCH_SIZE)\n",
        "        memory_batch = [(self.reshape_image(state), action, reward, np.zeros(shape=(1, *IMAGE_SHAPE), dtype=np.uint8), done) if done\n",
        "                        else (self.reshape_image(state), action, reward, self.reshape_image(next_state), done)\n",
        "                        for (state, action, reward, next_state, done) in memory_batch]\n",
        "\n",
        "        states = self.reshape_image([state for (state, *rest) in memory_batch], batch_size=MEMORY_BATCH_SIZE)\n",
        "        next_states = self.reshape_image([next_state for (_, _, _, next_state, _) in memory_batch], batch_size=MEMORY_BATCH_SIZE)\n",
        "\n",
        "        state_predictions = self.model.predict(states)\n",
        "        next_state_predictions = self.model.predict(next_states)\n",
        "        target_next_state_predictions = self.model.predict(next_states, is_target = True)\n",
        "\n",
        "        inputs = np.zeros(shape=(MEMORY_BATCH_SIZE, *IMAGE_SHAPE))\n",
        "        outputs = np.zeros(shape=(MEMORY_BATCH_SIZE, number_of_actions))\n",
        "\n",
        "        for i, (state, action, reward, next_state, is_done) in enumerate(memory_batch):\n",
        "            state_target = state_predictions[i].numpy() # Target Q(s,a) for state and action of sample i: [Q1 Q2 Q3 Q4] \n",
        "            next_state_target = target_next_state_predictions[i] \n",
        "            future_discounted_reward = target_next_state_predictions[i][tf.argmax(next_state_predictions[i])] # QTarget[nextstate][action]\n",
        "            state_target[action] = reward if is_done else reward + DISCOUNT_FACTOR * future_discounted_reward \n",
        "            inputs[i], outputs[i] = state, state_target\n",
        "\n",
        "        self.model.train(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    \"\"\"\n",
        "    Creates a game environment which an agent can play using certain actions.\n",
        "    Run takes an agent as argument that plays the game, until the agent 'dies' (no more lives)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, problem, is_video):\n",
        "        self.gym = gym.make(problem)\n",
        "        self.state_space = self.gym.observation_space.shape\n",
        "        self.frame_preprocessor = FramePreprocessor(self.state_space)\n",
        "        self.best_reward = 0\n",
        "        self.is_video = is_video\n",
        "        \n",
        "\n",
        "    # @tf.function\n",
        "    def clip_reward(self, reward):  # Clip positive rewards to 1 and negative rewards to -1\n",
        "        return np.sign(reward) #tf.sign(reward)\n",
        "\n",
        "    def display_gif(self):\n",
        "        self.img.set_data(self.gym.render('rgb_array')) # just update the data\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "    def display_video(self):\n",
        "        video = io.open('./gym-results/openaigym.video.%s.video000000.mp4' % self.gym.file_infix, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        HTML(data='''<video width=\"360\" height=\"auto\" alt=\"test\" controls><source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" /></video>'''\n",
        "            .format(encoded.decode('ascii')))\n",
        "\n",
        "    def setup_renderer(self):\n",
        "        if self.is_video:\n",
        "            self.gym  = wrappers.Monitor(self.gym, \"./gym-results\", force=True) # Video generator\n",
        "        else:\n",
        "            self.img = plt.imshow(self.gym.render('rgb_array')) # Inline image\n",
        "\n",
        "    def run(self, agent, should_print, should_render):\n",
        "        total_reward, step = 0, 0\n",
        "        if should_render: # Warming up display without preprocessing\n",
        "            print('Pausing training and let agent play for one episode.')\n",
        "            self.setup_renderer()\n",
        "\n",
        "        state = self.gym.reset()\n",
        "        state = self.frame_preprocessor.preprocess_frame(state)\n",
        "\n",
        "        while True:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, is_done, _ = self.gym.step(action)\n",
        "            # reward = self.clip_reward(reward) # Only for generalization to other Atari games\n",
        "\n",
        "\n",
        "            if not should_render or self.is_video: # Train agent\n",
        "                next_state = self.frame_preprocessor.preprocess_frame(next_state) # Preprocess\n",
        "\n",
        "                if is_done: next_state = None\n",
        "                experience = (state, action, reward, next_state, is_done)  # Experience(experience)\n",
        "                agent.observe(experience)\n",
        "\n",
        "                if agent.experiences.size > REPLAY_START_SIZE: # SPEED UP BY TRAINING ONLY EVERY 50th STEP and step < 50:\n",
        "                    agent.replay()  # Train on states in mini batches\n",
        "\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                step += 1\n",
        "\n",
        "            else: \n",
        "                self.display_gif()\n",
        "            \n",
        "            if is_done: break\n",
        "\n",
        "\n",
        "\n",
        "        self.best_reward = total_reward if total_reward > self.best_reward else self.best_reward\n",
        "        self.gym.close()\n",
        "        \n",
        "        # if self.is_video:\n",
        "        #     self.display_video()\n",
        "        \n",
        "        if should_print:\n",
        "            print(f\"Total reward: {total_reward} memory: {agent.experiences.size} exploration rate: {agent.exploration_rate} \\n\")\n",
        "        if should_render:\n",
        "            print(f'Play time over. Back to training.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Total reward: 2.0 memory: 4523 exploration rate: 0.9837172000000339 \n\nEpisode: 25 - Current overall best reward: 3.0\nTotal reward: 1.0 memory: 8965 exploration rate: 0.9677260000000673 \n\nEpisode: 50 - Current overall best reward: 4.0\nTotal reward: 0.0 memory: 13383 exploration rate: 0.9518212000001004 \n\nEpisode: 75 - Current overall best reward: 7.0\n"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-de8ded8ba3df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mshould_print\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m25\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mshould_render\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_print\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshould_print\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Episode: {episode+1} - Current overall best reward: {environment.best_reward}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-8-3dc65d088264>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, agent, should_print, should_render)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[1;31m# reward = self.clip_reward(reward) # Only for generalization to other Atari games\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\Dennis\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\Dennis\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Users\\Dennis\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "environment = Environment(PROBLEM, is_video = True)\n",
        "number_of_states = environment.gym.observation_space.shape\n",
        "number_of_actions = environment.gym.action_space.n\n",
        "dqn_agent = Agent(number_of_states, number_of_actions)  \n",
        "\n",
        "for episode in range(NUMBER_OF_EPISODES):\n",
        "    should_print = (episode + 1) % 25 == 0\n",
        "    should_render = (episode + 1) % 30 == 0\n",
        "    environment.run(dqn_agent, should_print, should_render)\n",
        "    if should_print:\n",
        "        print(f\"Episode: {episode+1} - Current overall best reward: {environment.best_reward}\")\n",
        "\n",
        "# TODO: 1) Convert NP to Tensors 2) Create Q Target network 3) Store model parameters 4) Run experiments!!! \n",
        "# Report: What did you implement. The experiments, difficulties (local machines, scalability, less episodes and memory) and results. Last 2-3 hours with less experiments. 14 pages \n",
        "# Images of architecture, Breakout, convolutions, preprocessed images, Tables of results (time, reward, exploration rate, episodes, memory, hyperparams)\n",
        "# Intro: Paper 1-2 page Objective, Theory behind CNN and Reinforcement Q Learning and Deep Q Learning 3 pages, Implementation 2 pages, Experiments and Results 2 pages, Discuss Improvements/Conclusion 1 page    \n",
        "# Improvements: faster machine, scalable optimizations, run with more games, generalize to other games? we run for Breakout but not for generalization \n",
        "# Technical: CNN architecture, experience replay, Q target network, \n",
        "# 500 episodes play randomly, train 300 episodes, env.render every100th episode and repeat training after, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eHZrymH45oDM"
      },
      "outputs": [],
      "source": []
    }
  ]
}